## using mini-batches

## what are the issues with large models?

- do not have the NN deeper than it's needed.

- activation function change to ReLu

  - Allows hidden layer to output 0

- typical too deep?

- typical too shallow?

- explicit drop out

  - randomly turn off a node during training
    - stop overfitting
    - certain randomness

  ## designing deep learning architectures

- PyTorch
- tensor -> matrix?

- FC -> 
- Forward function in classifier

